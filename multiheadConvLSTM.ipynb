{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multihead.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MokDvHl1HFAhVugtOjDLPPp6B9XTDkBy",
      "authorship_tag": "ABX9TyN5b8hGGzFZcsiiyYgtg2k2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dheerajshah13/Human-Activity-Recognition/blob/main/multiheadConvLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9l3wKpLh8xB"
      },
      "source": [
        "from numpy import mean,std,dstack\n",
        "from pandas import read_csv\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input,Dense,Flatten,Dropout,ConvLSTM2D,MaxPool3D\n",
        "from keras.layers.merge import concatenate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-T89j5HktaA"
      },
      "source": [
        "def load_file(filepath):\n",
        "  dataframe = read_csv(filepath,header =None,delim_whitespace =True)\n",
        "  return dataframe.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmIgupnolAQS"
      },
      "source": [
        "#for RAW data\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data)\n",
        "  loaded = dstack(loaded)\n",
        "  return loaded\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVFT-HzNlbcC"
      },
      "source": [
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + group +'/Inertial Signals/'\n",
        "  filenames = list()\n",
        "  filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt','total_acc_z_'+group+'.txt']\n",
        "  filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt','body_acc_z_'+group+'.txt']\n",
        "  filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt','body_gyro_z_'+group+'.txt']\n",
        "\n",
        "  X = load_group(filenames,filepath)\n",
        "  y = load_file(prefix + group + '/y_'+group+'.txt') \n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYbjBXcPmMU0"
      },
      "source": [
        "def load_dataset(prefix=''):\n",
        "  trainX, trainy = load_dataset_group('train', prefix + '/content/drive/MyDrive/HAR/HARDataset/')\n",
        "  testX, testy = load_dataset_group('test', prefix + '/content/drive/MyDrive/HAR/HARDataset/')\n",
        "  trainy = trainy - 1\n",
        "  testy = testy - 1\n",
        "  trainy = to_categorical(trainy) \n",
        "  testy = to_categorical(testy) \n",
        "  return trainX, trainy, testX, testy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57UupYtqmSLO"
      },
      "source": [
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  verbose,epochs,batch_size = 0,25,64\n",
        "  n_features, n_outputs = trainX.shape[2],trainy.shape[1]\n",
        "  n_steps, n_length = 4,32\n",
        "  trainX = trainX.reshape((trainX.shape[0],n_steps,1,n_length,n_features))\n",
        "  testX = testX.reshape((testX.shape[0],n_steps,1,n_length,n_features))\n",
        "\n",
        "  input1 = Input(shape=(n_steps,1,n_length,n_features))\n",
        "  cvls1 = ConvLSTM2D(64,(1,3),activation='relu',return_sequences=True, padding='valid')(input1)\n",
        "  drop1 = Dropout(0.5)(cvls1)\n",
        "  pool1 = MaxPool3D(pool_size=(1,1,2))(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  input2 = Input(shape=(n_steps,1,n_length,n_features))\n",
        "  cvls2 = ConvLSTM2D(96,(1,5),activation='relu',return_sequences=True, padding='valid')(input2)\n",
        "  drop2 = Dropout(0.5)(cvls2)\n",
        "  pool2 = MaxPool3D(pool_size=(1,1,2))(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  input3 = Input(shape=(n_steps,1,n_length,n_features))\n",
        "  cvls3 = ConvLSTM2D(96,(1,7),activation='relu',return_sequences=True, padding='valid')(input3)\n",
        "  drop3 = Dropout(0.5)(cvls3)\n",
        "  pool3 = MaxPool3D(pool_size=(1,1,2))(drop3)\n",
        "  flat3  = Flatten()(pool3)\n",
        "\n",
        "  merged = concatenate([flat1,flat2,flat3])\n",
        "\n",
        "  dense1 = Dense(100,activation ='relu')(merged)\n",
        "  dense2 = Dense(64,activation = 'relu')(dense1)\n",
        "  outputs = Dense(n_outputs,activation='softmax')(dense2)\n",
        "  \n",
        "  model = Model(inputs = [input1,input2,input3], outputs = outputs)\n",
        "  plot_model(model,show_shapes=True, to_file='model_arch.png')\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.fit([trainX,trainX,trainX], trainy, epochs=epochs, batch_size=batch_size,verbose=verbose)\n",
        "  _, accuracy = model.evaluate([testX,testX,testX], testy, batch_size=batch_size, verbose=0)\n",
        "  return accuracy\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eR5MNaDnLld"
      },
      "source": [
        "def summarize_results(scores):\n",
        "  print(scores)\n",
        "  m, s = mean(scores), std(scores) \n",
        "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekIcomMSrVSy"
      },
      "source": [
        "def run_experiment(repeats=10):\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    score = evaluate_model(trainX, trainy, testX, testy) \n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  summarize_results(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huH-WQZSrXcR",
        "outputId": "d240c7c4-b478-496a-aab8-bd3cf7ebfa09"
      },
      "source": [
        "run_experiment()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">#1: 91.822\n",
            ">#2: 93.519\n",
            ">#3: 92.026\n",
            ">#4: 91.890\n",
            ">#5: 92.094\n",
            ">#6: 92.094\n",
            ">#7: 91.992\n",
            ">#8: 91.686\n",
            ">#9: 90.804\n",
            ">#10: 89.549\n",
            "[91.82218909263611, 93.5188353061676, 92.02578663825989, 91.89005494117737, 92.09365248680115, 92.09365248680115, 91.99185371398926, 91.68646335601807, 90.80420732498169, 89.54869508743286]\n",
            "Accuracy: 91.748% (+/-0.963)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-WfnztZYBBs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}